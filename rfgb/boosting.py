
# Copyright (C) 2017-2018 RFGB Contributors
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program (at the base of this repository). If not,
# see <http://www.gnu.org/licenses/>

"""
Core methods for performing learning and inference, such as computing
gradients, updating gradients, and performing inference.

Documentation
-------------
"""

from __future__ import division

from .utils import Utils
from .logic import Prover

from math import log
from math import exp
from copy import deepcopy

__logPrior__ = log(0.5/float(1-0.5))

"""
class Boosting(object):
    '''boosting class'''

    # Wouldn't `logPrior = 0.0` be easier?
    logPrior = log(0.5/float(1-0.5))
"""


def computeAdviceGradient(example):
    """
    Proves each clause (:meth:`.Prover.prove`) and computes the advice gradient
    as ``NumberTrue - NumberFalse``.

    :param example:
    :type example:
    """

    nt, nf = 0, 0
    target = Utils.data.target.split('(')[0]
    for clause in Utils.data.adviceClauses:
        if Prover.prove(Utils.data, example, clause):
            if target in Utils.data.adviceClauses[clause]['preferred']:
                nt += 1
            if target in Utils.data.adviceClauses[clause]['nonPreferred']:
                nf += 1
    return (nt-nf)


def computeSumOfGradients(example, trees, data):
    """
    Computes new gradients for an example.

    :param example:
    :type example:

    :param trees:
    :type trees:

    :param data:
    :type data:
    """

    sumOfGradients = 0

    # Add leaf values satisfied by example in each tree.
    for tree in trees:
        gradient = inferTreeValue(tree, example, data)
        sumOfGradients += gradient
    return sumOfGradients


def inferTreeValue(clauses, query, data):
    """
    Returns the probability of `query` given data and the clauses learned.

    :param clauses:
    :type clauses:

    :param query:
    :type query:

    :param data:
    :type data:
    """
    for clause in clauses:

        clauseCopy = deepcopy(clause)
        clauseValue = float(clauseCopy.split(' ')[1])
        clauseRule = clauseCopy.split(' ')[0].replace(';', ',')

        if not clauseRule.split(':-')[1]:
            return clauseValue
        if Prover.prove(data, query, clauseRule):
            # Check if query satisifes clause
            return clauseValue


def performInference(testData, trees):
    """
    Computes the probabilities for test examples.

    :param testData: Data for testing.
    :type testData: :py:class:`.utils.Data` object.

    :param trees: List of strings representing learned decision trees.
    :type trees: list.

    Example:

    .. code-block:: python

                    from rfgb.boosting import performInference

    """

    logPrior = __logPrior__
    if not testData.regression:

        # Initialize log odds of assumed prior probability for example.
        logPrior = log(0.5/float(1-0.5))
        for example in testData.pos:

            # Compute sum of gradients
            sumOfGradients = computeSumOfGradients(example, trees, testData)

            # Calculate probability as sigmoid(log odds)
            testData.pos[example] = Utils.sigmoid(logPrior + sumOfGradients)

        for example in testData.neg:

            # Compute sum of gradients
            sumOfGradients = computeSumOfGradients(example, trees, testData)

            # Calculate probability as sigmoid(log odds)
            testData.neg[example] = Utils.sigmoid(logPrior + sumOfGradients)

    elif testData.regression:
        for example in testData.examples:
            sumOfGradients = computeSumOfGradients(example, trees, testData)
            testData.examples[example] = sumOfGradients


def updateGradients(data, trees, loss='LS', delta=None):
    """
    Update gradients of the data.

    :param data: Training or testing data (with parameters).
    :type data: :py:class:`.utils.Data` object.

    :param trees: List of strings representing trees.
    :type trees: list.

    :param loss: Loss function for regression (currently implemented:
                 'LS', 'LAD', 'Huber').
    :type loss: str.

    :param delta: Delta value for Huber loss.
    :type delta: float

    Example:

    .. code-block:: python

                    from rfgb.boosting import updateGradients

    """

    if data.regression:
        # If this is regression data, compute gradient as y - y_hat

        for example in data.examples:
            sumOfGradients = computeSumOfGradients(example, trees, data)
            trueValue = data.getExampleTrueValue(example)

            if loss == "LS":
                # Least Squares
                data.examples[example] = trueValue - sumOfGradients

            elif loss == "LAD":
                # Least Absolute Deviation
                updatedGradient = 0
                gradient = trueValue - sumOfGradients
                if gradient:
                    updatedGradient = gradient/float(abs(gradient))
                data.examples[example] = updatedGradient

            elif loss == "Huber":
                # Huber Loss
                gradient = trueValue - sumOfGradients
                updatedGradient = 0
                if gradient:
                    if gradient > float(delta):
                        updatedGradient = gradient/float(abs(gradient))
                    elif gradient <= float(delta):
                        updatedGradient = gradient
                data.examples[example] = updatedGradient

    else:
        # If this is classification data, compute 1 - P for each positive.
        """
        logPrior = __logPrior__

        for example in data.pos:
            sumOfGradients = computeSumOfGradients(example, trees, data)
        """

        logPrior = __logPrior__

        # P = sigmoid(sum of gradients) given by each tree learned so far.

        for example in data.pos:
            # For each positive example compute 1 - P
            sumOfGradients = computeSumOfGradients(example, trees, data)
            probabilityOfExample = Utils.sigmoid(logPrior + sumOfGradients)
            updatedGradient = 1 - probabilityOfExample
            if data.advice:
                adviceGradient = computeAdviceGradient(example)
                updatedGradient += adviceGradient
            data.pos[example] = updatedGradient

        for example in data.neg:
            # For each negative example compute 0 - P
            sumOfGradients = computeSumOfGradients(example, trees, data)
            probabilityOfExample = Utils.sigmoid(logPrior + sumOfGradients)
            updatedGradient = 0 - probabilityOfExample
            if data.advice:
                adviceGradient = computeAdviceGradient(example)
                updatedGradient += adviceGradient
            data.neg[example] = updatedGradient
